---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Joshua Nahm (jkn557)

### Introduction 

The dataset that was selected contains exam and course grade data from a statistics class. This data was retrieved from the Github source from the vincentarelbundock website. Within this dataset, there are a total of 7 variables: `number`, `semester`, `sex`, `exam1`, `exam2`, `exam3`, and `course_grade`. The variables that will be specifically investigated are `sex`, `exam1`, `exam2`, `exam3`, and `course_grade` with `sex` serving as the binary variable. The `exam1`, `exam2`, and `exam3` variables are numeric variables that indicate the scores of the respective exams for each student during the semester. On the other hand, the `course_grade` variable represents the final grade of the student. In total, there are 232 observations, and there are 187 observations for the Man group and 45 observations for the Woman group.

```{R}
library(tidyverse)
# read your datasets in here, e.g., with read_csv()
grades <- read_csv("exam_grades.csv")
grades <- grades %>% select(1, 3:7) %>% na.omit
# if your dataset needs tidying, do so here

# any other code here
glimpse(grades)
grades %>% group_by(sex) %>% count()
```

### Cluster Analysis

```{R}
library(cluster)
library(GGally)
# clustering code here
dat1 <- grades %>% select(exam1, exam2, exam3, course_grade)
sil_width <- vector()
for (i in 2:10) {
    pam_fit <- pam(dat1, k = i)
    sil_width[i] <- pam_fit$silinfo$avg.width
}
ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)

grades_pam <- dat1 %>% pam(k = 2)
plot(grades_pam, which = 2)
grades_pam$silinfo$avg.width

grades %>% slice(grades_pam$id.med)

grades_clus <- grades %>% mutate(cluster = factor(grades_pam$clustering))
ggpairs(grades_clus, cols = 2:5, aes(color = cluster))
```

Initially, after using the ggplot function, it was determined that the number of clusters in this dataset was 2. In terms of overall average silhouette width, the average silhouette width was found to be approximately 0.34, which is considered to be a weak structure and could be considered artificial as it falls within the range of 0.26 and 0.50. Between the two clusters, the first cluster is wider than the second. The `sex` Man serves as both medoids. They are most similar on `exam1` and most different on `exam3`. 

Based on the function ggpairs, `exam3` appears to show the greatest difference between clusters, and `exam1` appears to show the least difference between clusters. Cluster 2, or the blue cluster, appears to have lesser `exam1`, lesser `exam2`, lesser `exam3`, and lesser `course_grade`.

    
    
### Dimensionality Reduction with PCA

```{R}
# PCA code here
grades %>% select(3:6) -> grades_select
princomp(grades_select, cor=T) -> pca1
summary(pca1, loadings = T)

eigval <- pca1$sdev^2
varprop = round(eigval / sum(eigval), 2)

ggplot() + geom_bar(aes(y=varprop, x=1:4), stat="identity") + xlab("") + geom_path(aes(y=varprop, x=1:4)) + 
  geom_text(aes(x=1:4, y=varprop, label=round(varprop, 2)), vjust=1, col="white", size=5) + 
  scale_y_continuous(breaks=seq(0, .6, .2), labels = scales::percent) + 
  scale_x_continuous(breaks=1:10)

round(cumsum(eigval) / sum(eigval), 2)
eigval

cor(pca1$scores) %>% round(10)

grades %>% as.data.frame %>% select(studentid) %>% mutate(PC1 = pca1$scores[, 
    1], PC2 = pca1$scores[, 2]) %>% ggplot(aes(PC1, 
    PC2, color = studentid)) + geom_point()
```

Based on the rules of thumb, we can assume that the first two PCs will be retained. Although it should be noted that the second PC does not have an eigenvalue greater than 1 as seen by Kaiser's rule. The first two PCs represent 82% of the total variance. The loadings for PC1 indicate that a higher scores on PC1 lead to higher scores in `exam1`, `exam2`, `exam3`, and `course_grade`. Higher scores on PC2 indicate higher scores in `exam1` and lower scores in `exam2` and `exam3`. 

Based on the graph as a whole, it appears that there is not a correlation between PC1 and PC2. However, it also appears that the individuals with higher student IDs have higher grades in PC2, and the individuals with lower student IDs have lower grades in PC1.

###  Linear Classifier

```{R}
# linear classifier code here
logistic_fit <- glm(sex=="Man" ~ exam1 + exam2 + exam3 + course_grade, data = grades, family="binomial")
logistic_fit

prob_reg <- predict(logistic_fit, type = "response")
class_diag(prob_reg, truth = grades$sex, positive = "Man")

y <- grades$sex
y <- factor(y, levels=c("Man","Woman"))
y_hat <- sample(c("Man","Woman"), size=length(y), replace=T)

y_hat <- factor(y_hat, levels=c("Man","Woman"))
table(actual=y, predicted = y_hat) %>% addmargins
```

```{R}
# cross-validation of linear classifier here
k=10
data<-sample_frac(grades)
folds <- rep(1:10, length.out=nrow(data))

diags<-NULL

i=1
for(i in 1:k){
train<-data[folds!=i,] 
test<-data[folds==i,] 
truth<-test$sex

fit <- glm(as.factor(sex) ~ exam1 + exam2 + exam3 + course_grade, data = train, family = "binomial")

probs <- predict(fit, newdata = test, type = "response")

diags<-rbind(diags,class_diag(probs,truth, positive="Man")) }

summarize_all(diags,mean)
```

In this dataset, the binary variable was `sex`, so the response was predicted from the variables `exam1`, `exam2`, `exam3`, and `course_grade`. Utilizing the glm function in linear classification, an AUC value of 0.6746 was calculated. Such a value results in a poor interpretation because it falls within the range 0.6 to 0.7. This indicates that sex is poor in differentiating between `exam1`, `exam2`, `exam3`, and `course_grade`. On the other hand, when training the model to the entire dataset through a cross-validation of the linear classifier, the AUC value becomes 0.436, which results in a worse interpretation than the former as it is below the range of 0.5 to 0.6. To continue, after perfoming the k-fold CV, there appears to be a noticeable decrease in AUC when predicting out of sample. Because there is a significant decrease in AUC values from 0.6746 to 0.436, it is evident that there are signs of overfitting.

### Non-Parametric Classifier

```{R}
library(caret)
# non-parametric classifier code here
knn3(sex=="Man" ~ exam1 + exam2 + exam3 + course_grade, data=grades)

knn_fit <- knn3(factor(sex=="Man",levels=c("TRUE","FALSE")) ~ exam1 + exam2 + exam3 + course_grade, data=grades, k=5)
y_hat_knn <- predict(knn_fit,grades)
table(truth = factor(grades$sex=="Man", levels=c("TRUE","FALSE")),
  prediction= factor(y_hat_knn[,1]>.5, levels=c("TRUE","FALSE")))

prob_knn <- predict(knn_fit, newdata = grades)
class_diag(prob_knn[,2], truth = grades$sex, positive = "Man")

```

```{R}
# cross-validation of np classifier here
k=10

data<-sample_frac(grades)
folds <- rep(1:k, length.out=nrow(data))

diags<-NULL

i=1
for(i in 1:k){
  train<-data[folds!=i,] 
  test<-data[folds==i,] 
  truth<-test$sex

fit <- knn3(sex~exam1 + exam2 + exam3 + course_grade, data = train)

probs <- predict(fit, newdata = test)[,2]

diags<-rbind(diags,class_diag(probs,truth, positive="Man")) }

summarize_all(diags,mean)
```

Per an AUC value of 0.2446, the model appears to performing very badly because it falls below the range of 0.5 to 0.6. When predicting out of sample and after cross-validation of the non-parametric classifier, there does not appear a significant decrease but rather a significant increase from the AUC value of 0.2446 to an AUC value of 0.53459. This suggests that there were not any signs of overfitting. However, though there were improvements, an AUC value of 0.53459 is still considered bad as it is within the range of 0.5 to 0.6. Furthermore, when compared with the linear model in its cross-validation performance, the non-parametric model appears to be performing worse as the AUC values generally appear to be less. 


### Regression/Numeric Prediction

```{R}
# regression model code here
fit <- lm(course_grade~exam1 + exam2 + exam3,data=grades)
yhat<-predict(fit) 
mean((grades$course_grade-yhat)^2) 
```

```{R}
# cross-validation of regression model here
set.seed(1234)
k=5

data<-grades[sample(nrow(grades)),]
folds<-cut(seq(1:nrow(grades)),breaks=k,labels=F)

diags<-NULL

for(i in 1:k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  fit<-lm(course_grade~.,data=train)
  yhat<-predict(fit,newdata=test)
  diags<-mean((test$course_grade-yhat)^2)
}
mean(diags) 
```

Prior to cross-validation with linear regression, the mean squared error (MSE) was found to be 18.41592, and after the cross-validation with linear regresssion, the MSE was found to be 17.25959. Both values seem to be relatively small, which indicates a lower prediction error within this dataset. Additionally, because the MSE is lower after cross-validation, there does not appear to be evidence that points toward an overfitting of the model.

### Python 

```{R}
library(reticulate)
use_python("/usr/bin/python3", required = F)

final_average <- grades %>% summarize((py$course_average + mean(exam1) + mean(exam2) + mean(exam3)) / 4)
final_average #this value does not show in the website due to the long calculation code, but the calculated value is 75.31271.
```

```{python}
# python code here
import numpy as np
grades = r.grades
course_average = np.mean(grades["course_grade"])
course_average
```

For the chunk of Python code, a function in r, reticulate, was utilized in order to interactively use both R and Python by creating a variable called `course_average` that calculates the average grade among all students at the end of the class. In the chunk of R code, this `course_average` variable was shared from the Python code in order to find the mean of all of the mean grades for each exam and final course grade, or `final_average`.

### Concluding Remarks

In this project, I utilized a dataset in order to make some calculations and analyses utilizing topics from clustering, dimensionality reduction, linear and non-parametric classifiers, cross-validations, regression/predictions, and Python. It was incredibly interesting to see how analyzing a simple dataset as exam grades in a class can yield such interesting results!




